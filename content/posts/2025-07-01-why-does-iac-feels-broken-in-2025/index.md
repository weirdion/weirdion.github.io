+++
title = "Why Infrastructure as Code Feels Broken in 2025"
description = "Discover why Infrastructure as Code tools like Terraform, CDK, and Pulumi still feel fragile and complex, plus solutions for better IaC workflows."
date = 2025-07-01
categories = ["Infrastructure", "AWS", "Cloud", "DevOps"]
tags = ["iac", "aws", "cdk", "terraform", "pulumi", "cloud", "devops", "multi-cloud", "state-management"]
draft = false
canonical = "https://weirdion.github.io/posts/2025-07-01-why-does-iac-feels-broken-in-2025/"
+++

<!-- Schema.org structured data -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Why Infrastructure as Code Feels Broken in 2025",
  "description": "Discover why Infrastructure as Code tools like Terraform, CDK, and Pulumi still feel fragile and complex, plus solutions for better IaC workflows.",
  "image": "https://weirdion.github.io/assets/images/2025-07-01-why-does-iac-feels-broken-in-2025.jpg",
  "author": {
    "@type": "Person",
    "name": "Ankit Patterson"
  },
  "datePublished": "2025-07-01",
  "mainEntityOfPage": "@id"
}
</script>

Iâ€™ve worked with infrastructure in one form or another for years now â€” from click-ops to CDK, Pulumi, and Terraform. The tooling has definitely evolved, but so has the complexity. And lately, Iâ€™ve found myself asking a frustrating question more and more often:

**Why does infrastructure as code still feel soâ€¦ brittle?**

We have more tools than ever. And yet, getting a clean, reliable, and collaborative setup across teams still takes effort, workarounds, and too much time fiddling with low-level quirks. Iâ€™m not trying to be cynical or trying to dunk on any one tool. But as an engineer, I canâ€™t shake the feeling that our current abstractions are reaching their limits.

![AI generated image of developers trying to scaffold a building of blocks into the cloud with message bubbles like API Breakage, State and IAM mess visible around them.](featured.jpg)


## **ğŸš§ The Friction in Todayâ€™s Tooling**

Weâ€™re not short on options. What weâ€™re short on is composability, predictability, and confidence. Below are the pain points that show up again and again in real-world infrastructure work.

### **ğŸŒ¥ï¸ The False Promise of Multi-Cloud**

Every provider has its own quirks, service limits, and naming inconsistencies. Even the simplest concept â€” like a â€œbucketâ€ â€” behaves differently across AWS, GCP, and Azure.

Tools like Terraform or Pulumi claim multi-cloud support, but under the hood theyâ€™re often just stitching together individual provider SDKs. The abstraction is shallow. Try modeling advanced IAM conditions or network boundaries generically â€” youâ€™ll quickly find yourself writing:

```
if (cloud === 'aws') {
  // do X
} else if (cloud === 'gcp') {
  // do Y
}
```

Thatâ€™s not abstraction â€” itâ€™s conditional logic.

**The result?**Â Codebases become brittle, hard to test, and difficult to scale across environments.

### **ğŸ—ƒï¸ State Management Is Still Fragile**

Remote state is powerful â€” and also a pain.

With Terraform, youâ€™re setting up resources like S3 buckets, DB for locks, IAM roles to access them. Pulumi simplifies some of thatâ€¦ but unless you self-host, youâ€™re locked into their cloud state backend.

Even CDK, which feels more native to developers, just compiles down to CloudFormation stacks. So you inherit CloudFormationâ€™s limitations â€” like slow deploys, drift detection weirdness, and nested stack complexity.

**State is both essential and fragile**. And almost every real-world team Iâ€™ve worked with has had to fight it.

### **ğŸ§± You Canâ€™t Escape the Bootstrap**

Even with the best tooling, thereâ€™s always a chicken-and-egg problem:

> **_Where does your infrastructure state liveâ€¦ before you have infrastructure?_**

That means pre-creating resources to store, access and manage the state. And yes, there are managed â€œCloudâ€ services, so you either accept vendor lock-in or you self-host, and accept the issues as reasonable cost.

Itâ€™s a quiet contradiction:Â **you canâ€™t fully do infrastructure-as-code, because your infrastructure needs somewhere to store its code state first**.

Some teams hack around this with shell scripts, Terraform â€œbootstrapâ€ folders, or even click-ops buckets with README instructions. But it breaks the illusion of pure declarative infrastructure â€” and becomes one more thing to track, and for disaster recovery, itâ€™s another thing thatâ€™s a big blip on the radar.

### **ğŸ§ª Testing and Validation (orâ€¦ Governance?)**

Testing infrastructure code goes well beyond â€œdoes this syntax work?â€ â€” itâ€™s about ensuring that what you deploy matches what you intended: least privilege, encrypted data, proper network boundaries, etc.

But thatâ€™s still hard.

- **Unit tests**Â via CDK assertions, Pulumi mocks, or kitchen-terraform oftenÂ **validate structure, not intent**.
    
- **Policy tools**Â like OPA (Rego), Checkov bolt on compliance enforcement, but feel external â€” and often trigger late in the CI/CD pipeline.
    

#### **ğŸ‘” Governance as a workaround**

Many larger teams try to codify best practices as reusable building blocks, usually wrapped in higher-level abstractions:

- **CDK Level 3 constructs**: Opinionated patterns that bundle multiple resources with sane defaults (e.g., a secure S3 bucket with logging, encryption, and versioning).
    
- **Pulumi Best Practices Library**: Attempting to encode common security and compliance expectations into standard building blocks.
    
- **Terraform Modules:**Â Often wrapped internally with pre-set variables and guardrails to prevent insecure defaults.
    

These efforts helpÂ **reduce footguns**, but they donâ€™t remove complexity â€” they just relocate it. And thereâ€™s often a silent tension between flexibility and safety.

Tooling vendors try to walk a fine line: offeringÂ **low-level control**Â for power users, while promotingÂ **higher-level abstractions**Â that bake in best practices. But in practice, the higher-level constructs are oftenÂ **unknown or underused**by those writing day-to-day infrastructure â€” either due to lack of awareness, discoverability, or confidence in how opinionated those abstractions are.

And so we circle back â€” teams still end up writing raw IAM policies, managing subnets manually, and re-solving solved problems, one YAML file at a time.

### **ğŸ§¨ SDK Drift & Provider Upgrades Are Risky**

Cloud providers move fast â€” new features, new constraints, new behaviors.

Your IaC toolâ€™s SDK or plugin has to catch up. Sometimes that takes weeks. And when youÂ _do_Â upgrade, your once-working stack might break.

- CDKâ€™s construct libraries go stale if your team doesnâ€™t stay on top of things.
    
- Pulumi relies on upstream SDKs, so lag is common.
    
- Terraform providers have notorious upgrade issues â€” even minor bumps.
    

Infra code becomes a moving target. Not because your system changed â€” but because theÂ **tooling around it did**.

### **ğŸ¢ The Feedback Loop Is Way Too Slow**

Hereâ€™s the usual cycle:

1. Write Code
    
2. Plan (Â `cdk synth`Â /Â `terraform plan`Â /Â `pulumi preview`Â )
    
3. Deploy
    
4. Wait
    
5. Realize somethingâ€™s wrong
    
6. Try to roll back
    
7. Re-plan
    

Even if youâ€™re using preview features or dry runs, you canâ€™t see the impact of a change (latency, throughput, IAM restrictions, quotas) until after youâ€™ve applied it.

- **CDK**Â generates Cloudformation templates (Â `cdk synth`Â ), but you are still at the mercy of CFâ€™s slow deploys and rollback behavior.
    
- **Pulumi**Â addsÂ `pulumi preview`Â , but the preview doesnâ€™t always reflect what cloud providers will actually do at deploy time.
    
- **Terraform**Â gives you aÂ `terraform plan`, but it wonâ€™t catch everything like runtime errors, API limits.
    

In all cases, you often donâ€™t find out somethingâ€™s broken until after a real deploy:

- A bucket name conflict
    
- An API rate limit hit
    
- A security misconfiguration
    
- A resource being â€œin useâ€ or in the wrong state
    

Even when the tooling gives you a preview, it rarely gives youÂ **real-world feedback**Â about performance, latency, or cost â€” until you hit â€œdeployâ€ and find out the hard way. With app code, tests and previews are instant. With infra code, itÂ **often feels like guessing**.

*** End of venting, thanks for making it this far. ***

## **ğŸ§  Why This Keeps Happening**

Hereâ€™s the hard truth: most tools treat â€œinfrastructure as codeâ€ as aÂ **string templating problem**. You describe a desired state, and they generate the right JSON or YAML under the hood.

Tools likeÂ **Ansible**Â offered a kind of promise:Â _idempotent infrastructure_Â by inspecting actual system state before applying changes. If a user was already created, a package already installed, or a file already present, Ansible wouldnâ€™t touch it. You could rely on â€œwhat I asked for is what I getâ€ â€” at least on a single machine.

But infrastructure in the cloud world, that guarantee breaks down for a few reasons:

- You canâ€™t always see the full state
    
- Eventual consistency is everywhere
    
- Side effects, defaults, and hidden dependencies
    
- The API Isnâ€™t the whole story
    
- The Cloud is too big to model easily
    

Thatâ€™s too much to meaningfully wrap in a single syntax â€” and thatâ€™s where the abstraction starts to leak.

### **So Why Canâ€™t IaC Just Be Better?**

Because itâ€™s fighting an uphill battle. Unlike traditional config management, IaC operates in an opaque, distributed, multi-actor system:

- You donâ€™t control the OS
    
- You donâ€™t always get strong guarantees
    
- You donâ€™t see all the dependencies
    
- You canâ€™t always retry safely
    
- And the tooling often assumes you know more than you do
    

Weâ€™re asking tools to pretend infrastructure is deterministic when it often isnâ€™t.

## **ğŸ›¤ So Where Do We Go From Here?**

I donâ€™t have a perfect answer. But I do think weâ€™re overdue for a rethink.

What would a better system look like?

- **Composability:**Â Infra should be versioned and shared like code â€” without fighting registries or packaging quirks.
    
- **First-class policy:**Â Tools like OPA or Rego should be part of the IaC pipeline, not bolt-ons.
    
- **Infra-aware preview:**Â Show me what a change really does â€” not just a plan output.
    
- **Simulation environments:**Â Like test containers, but for cloud.
    
- **Multi-cloud-native design:**Â Not wrapper libraries, but shared core abstractions that degrade gracefully.
    

In short: letâ€™s build infrastructure tooling that understands infrastructure better.

Iâ€™m not here to trash the tools. Every one of them â€” CDK, Pulumi, Terraform â€” has helped me ship real systems. But Iâ€™ve also spent countless hours fighting their edges, debugging provider mismatches, or trying to reverse a bad deploy.

If youâ€™ve been there too â€” I see you.

Maybe the next era of infrastructure isnâ€™t just â€œmore tools,â€ butÂ **better ones**, built with empathy for the people who live in these trenches every day.
